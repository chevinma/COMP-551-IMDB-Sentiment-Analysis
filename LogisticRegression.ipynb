{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "D3Bv7kD612Uc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Initialize required lists, stemmers, stop words and punctuation removers."
      ]
    },
    {
      "metadata": {
        "id": "fg922kfs1QvQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "de54c597-f92f-448e-82d0-d60195bd373e"
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem import *\n",
        "import string\n",
        "\n",
        "# Use SnowballStemmer to stem input comments.\n",
        "ps = SnowballStemmer(\"english\")\n",
        "# Use nltk's predefined stopword list as our stop_words set.\n",
        "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "# Remove all occurrences of punctuation with this function.\n",
        "punct_remove = string.punctuation.maketrans('', '', string.punctuation)\n",
        "\n",
        "# Initiate list\n",
        "x_train = list()\n",
        "y_train = list()\n",
        "x_test = list()\n",
        "y_test = list()\n",
        "pos_list = list()\n",
        "\n",
        "x_real_test = list()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "owwSlYM12G93",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Read in training and test set."
      ]
    },
    {
      "metadata": {
        "id": "jBtSUuXH2NMz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Open Negative Training Data and append entries to list.\n",
        "for filename in os.listdir('./train/neg'):\n",
        "    file = open('./train/neg/' + filename, encoding=\"utf8\")\n",
        "    x_train.append(file.read())\n",
        "    y_train.append(0)\n",
        "    file.close()\n",
        "\n",
        "# Open Positive Training Data and append entries to list.\n",
        "for filename in os.listdir('./train/pos'):\n",
        "    file = open('./train/pos/' + filename, encoding=\"utf8\")\n",
        "    x_train.append(file.read())\n",
        "    y_train.append(1)\n",
        "    file.close()\n",
        "\n",
        "for i in range(25000):\n",
        "    file = open(\"./test/%d.txt\"%(i), encoding=\"utf8\")\n",
        "    x_real_test.append(file.read())\n",
        "    file.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s_Mdj3SA45vr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Function to process text before pipeline.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "qYslS-sF2Ymf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize as st, word_tokenize as wt\n",
        "import re\n",
        "\n",
        "'''\n",
        "Process text before pipeline.\n",
        "'''\n",
        "def stem_words(data, linebreak=False, notcontract=True,\n",
        "              havecontract=True, punctuation=True):\n",
        "\n",
        "    def feature_tokens(tokens):\n",
        "        stemtokens = list()\n",
        "        for i in range(len(tokens)):\n",
        "            if tokens[i] == 'not':\n",
        "                i += 1\n",
        "                continue\n",
        "            if tokens[i] not in stop_words:\n",
        "                stemmed = ps.stem(tokens[i])\n",
        "                if len(stemmed) > 2:\n",
        "                    stemtokens.append(stemmed)\n",
        "        return stemtokens\n",
        "\n",
        "    def processText(text, linebreak=False, notcontract=True, \n",
        "                    havecontract=True, punctuation=True):\n",
        "        if linebreak: text = re.sub(\"<.*>\", ' ', text)\n",
        "        if notcontract: text = re.sub(\"n't\", ' not', text)\n",
        "        if havecontract: text = re.sub(\"'ve\", ' have', text)\n",
        "        if punctuation: text = text.translate(punct_remove).lower()\n",
        "        return text\n",
        "\n",
        "    # initiate list for counting word frequencies in the list of documents\n",
        "    new_train = list()\n",
        "    for rawtext in data:\n",
        "        # remove line breaks, indenting, punctuation, contractions\n",
        "        text = processText(rawtext, linebreak, notcontract, havecontract,\n",
        "                          punctuation)\n",
        "\n",
        "        # adds all stems that aren't stopwords\n",
        "        tokens = wt(text)\n",
        "        stemtokens = feature_tokens(tokens)\n",
        "        new_train.append(' '.join(stemtokens))\n",
        "        \n",
        "    return new_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mV-dPg_T5gqs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Run pre-processing pipeline - model and generate predictions. Print metrics of performance on held-out validation set."
      ]
    },
    {
      "metadata": {
        "id": "Io4pUr1N5r-m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
        "import time\n",
        "\n",
        "prestart = time.time()\n",
        "new_train = stem_words(raw_x_train)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(new_train, y_train, train_size=0.8, test_size=0.2)\n",
        "\n",
        "\n",
        "pclf = Pipeline([\n",
        "    ('vect', CountVectorizer()),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('norm', Normalizer()),\n",
        "    ('clf', LogisticRegression()),\n",
        "])\n",
        "preend = time.time()\n",
        "\n",
        "fitstart = time.time()\n",
        "pclf.fit(X_train, y_train)\n",
        "fitend = time.time()\n",
        "\n",
        "predstart = time.time()\n",
        "y_pred = pclf.predict(X_test)\n",
        "predend = time.time()\n",
        "\n",
        "end = time.time()\n",
        "\n",
        "print(\"Pre-Process Time: {}\\nTraining Time: {}\\nPrediction Time: {}\".format(preend-prestart,\n",
        "                                                                            fitend-fitstart, predend-predstart))\n",
        "print(metrics.classification_report(y_test, y_pred))\n",
        "\n",
        "print(metrics.classification_report(y_test, y_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "siEOyNjkAdEw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression Tests\n",
        "\n",
        "In this test, we use tf-idf, a count vectorizer, and L2-Normalization. We also expand not and have contractions and remove punctuation but keep line breaks, before stemming the words. Note that we additionally scan for and disregard terms (bi-grams) of the form \"not x\".\n",
        "\n",
        "**We vary our values for C in our Logistic Regression**"
      ]
    },
    {
      "metadata": {
        "id": "blg0J57DA1nq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "new_train = stem_words(raw_x_train)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(new_train, y_train, train_size=0.8, test_size=0.2)\n",
        "\n",
        "for i in [0.01, 0.05, 0.25, 0.5, 0.6, 0.75, 1]:\n",
        "  \n",
        "    prestart = time.time()\n",
        "    pclf = Pipeline([\n",
        "        ('vect', CountVectorizer()),\n",
        "        ('tfidf', TfidfTransformer()),\n",
        "        ('norm', Normalizer()),\n",
        "        ('clf', LogisticRegression(C=i)),\n",
        "    ])\n",
        "    preend = time.time()\n",
        "\n",
        "    fitstart = time.time()\n",
        "    pclf.fit(X_train, y_train)\n",
        "    fitend = time.time()\n",
        "\n",
        "    predstart = time.time()\n",
        "    y_pred = pclf.predict(X_test)\n",
        "    predend = time.time()\n",
        "\n",
        "    end = time.time()\n",
        "    \n",
        "    print(\"C={}:\\nPre-Process Time: {}\\nTraining Time: {}\\nPrediction Time: {}\".format(i,preend-prestart,\n",
        "                                                                                fitend-fitstart, predend-predstart))\n",
        "    print(metrics.classification_report(y_test, y_pred))\n",
        "\n",
        "    print(metrics.classification_report(y_test, y_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6XDBXQOLDnQ2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**What happens if we turn off the IDF?**"
      ]
    },
    {
      "metadata": {
        "id": "8b-8gWs0Dtce",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "new_train = stem_words(raw_x_train)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(new_train, y_train, train_size=0.8, test_size=0.2)\n",
        "\n",
        "for i in [0.01, 0.05, 0.25, 0.5, 0.6, 0.75, 1]:\n",
        "  \n",
        "    prestart = time.time()\n",
        "    pclf = Pipeline([\n",
        "        ('vect', CountVectorizer()),\n",
        "        ('tfidf', TfidfTransformer(use_idf=False)),\n",
        "        ('norm', Normalizer()),\n",
        "        ('clf', LogisticRegression(C=i)),\n",
        "    ])\n",
        "    preend = time.time()\n",
        "\n",
        "    fitstart = time.time()\n",
        "    pclf.fit(X_train, y_train)\n",
        "    fitend = time.time()\n",
        "\n",
        "    predstart = time.time()\n",
        "    y_pred = pclf.predict(X_test)\n",
        "    predend = time.time()\n",
        "\n",
        "    end = time.time()\n",
        "    \n",
        "    print(\"C={}:\\nPre-Process Time: {}\\nTraining Time: {}\\nPrediction Time: {}\".format(i,preend-prestart,\n",
        "                                                                                fitend-fitstart, predend-predstart))\n",
        "    print(metrics.classification_report(y_test, y_pred))\n",
        "\n",
        "    print(metrics.classification_report(y_test, y_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7wxqrbXNGiMA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**What about if we use L1 Normalization when computing the TF-IDF vectors?**"
      ]
    },
    {
      "metadata": {
        "id": "5vhR9H-gGna8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "new_train = stem_words(raw_x_train)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(new_train, y_train, train_size=0.8, test_size=0.2)\n",
        "\n",
        "for i in [0.01, 0.05, 0.25, 0.5, 0.6, 0.75, 1]:\n",
        "  \n",
        "    prestart = time.time()\n",
        "    pclf = Pipeline([\n",
        "        ('vect', CountVectorizer()),\n",
        "        ('tfidf', TfidfTransformer(norm='l1')),\n",
        "        ('norm', Normalizer()),\n",
        "        ('clf', LogisticRegression(C=i)),\n",
        "    ])\n",
        "    preend = time.time()\n",
        "\n",
        "    fitstart = time.time()\n",
        "    pclf.fit(X_train, y_train)\n",
        "    fitend = time.time()\n",
        "\n",
        "    predstart = time.time()\n",
        "    y_pred = pclf.predict(X_test)\n",
        "    predend = time.time()\n",
        "\n",
        "    end = time.time()\n",
        "    \n",
        "    print(\"C={}:\\nPre-Process Time: {}\\nTraining Time: {}\\nPrediction Time: {}\".format(i,preend-prestart,\n",
        "                                                                                fitend-fitstart, predend-predstart))\n",
        "    print(metrics.classification_report(y_test, y_pred))\n",
        "\n",
        "    print(metrics.classification_report(y_test, y_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "coMDwHamG9af",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Let's now try to vary the minimum document frequency required for terms to be considered in our Count Vectorizer**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "0x5UQn3JHj8T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "new_train = stem_words(raw_x_train)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(new_train, y_train, train_size=0.8, test_size=0.2)\n",
        "\n",
        "for i in [0.005, 0.01, 0.015, 0.02, 0.025, 0.03, 0.035]:\n",
        "  \n",
        "    prestart = time.time()\n",
        "    pclf = Pipeline([\n",
        "        ('vect', CountVectorizer(min_df=i)),\n",
        "        ('tfidf', TfidfTransformer()),\n",
        "        ('norm', Normalizer()),\n",
        "        ('clf', LogisticRegression(C=1)),\n",
        "    ])\n",
        "    preend = time.time()\n",
        "\n",
        "    fitstart = time.time()\n",
        "    pclf.fit(X_train, y_train)\n",
        "    fitend = time.time()\n",
        "\n",
        "    predstart = time.time()\n",
        "    y_pred = pclf.predict(X_test)\n",
        "    predend = time.time()\n",
        "\n",
        "    end = time.time()\n",
        "    \n",
        "    print(\"Minimum DF={}:\\nPre-Process Time: {}\\nTraining Time: {}\\nPrediction Time: {}\".format(i,preend-prestart,\n",
        "                                                                                fitend-fitstart, predend-predstart))\n",
        "    print(metrics.classification_report(y_test, y_pred))\n",
        "\n",
        "    print(metrics.classification_report(y_test, y_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YsY2vqXxJBgS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**We now vary the upper bound on the n-grams analzed in the Count Vectorizer**"
      ]
    },
    {
      "metadata": {
        "id": "0E7sQOC7I5JM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "new_train = stem_words(raw_x_train)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(new_train, y_train, train_size=0.8, test_size=0.2)\n",
        "\n",
        "for i in [1, 2, 3]:\n",
        "  \n",
        "    prestart = time.time()\n",
        "    pclf = Pipeline([\n",
        "        ('vect', CountVectorizer(ngram_range(1,i))),\n",
        "        ('tfidf', TfidfTransformer()),\n",
        "        ('norm', Normalizer()),\n",
        "        ('clf', LogisticRegression(C=1)),\n",
        "    ])\n",
        "    preend = time.time()\n",
        "\n",
        "    fitstart = time.time()\n",
        "    pclf.fit(X_train, y_train)\n",
        "    fitend = time.time()\n",
        "\n",
        "    predstart = time.time()\n",
        "    y_pred = pclf.predict(X_test)\n",
        "    predend = time.time()\n",
        "\n",
        "    end = time.time()\n",
        "    \n",
        "    print(\"N-Gram Upperbound={}:\\nPre-Process Time: {}\\nTraining Time: {}\\nPrediction Time: {}\".format(i,preend-prestart,\n",
        "                                                                                fitend-fitstart, predend-predstart))\n",
        "    print(metrics.classification_report(y_test, y_pred))\n",
        "\n",
        "    print(metrics.classification_report(y_test, y_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BA8rSRa7Jbfj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**And we enforce a strict n-gram count for the Count Vectorizer:**"
      ]
    },
    {
      "metadata": {
        "id": "498u-RQfJX1v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "new_train = stem_words(raw_x_train)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(new_train, y_train, train_size=0.8, test_size=0.2)\n",
        "\n",
        "for i in [1, 2, 3]:\n",
        "  \n",
        "    prestart = time.time()\n",
        "    pclf = Pipeline([\n",
        "        ('vect', CountVectorizer(ngram_range(i,i))),\n",
        "        ('tfidf', TfidfTransformer()),\n",
        "        ('norm', Normalizer()),\n",
        "        ('clf', LogisticRegression(C=1)),\n",
        "    ])\n",
        "    preend = time.time()\n",
        "\n",
        "    fitstart = time.time()\n",
        "    pclf.fit(X_train, y_train)\n",
        "    fitend = time.time()\n",
        "\n",
        "    predstart = time.time()\n",
        "    y_pred = pclf.predict(X_test)\n",
        "    predend = time.time()\n",
        "\n",
        "    end = time.time()\n",
        "    \n",
        "    print(\"N-Gram Strict Count={}:\\nPre-Process Time: {}\\nTraining Time: {}\\nPrediction Time: {}\".format(i,preend-prestart,\n",
        "                                                                                fitend-fitstart, predend-predstart))\n",
        "    print(metrics.classification_report(y_test, y_pred))\n",
        "\n",
        "    print(metrics.classification_report(y_test, y_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eUIyNpL2J6EV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Let's go back and try our first configuration, but with removing Line Breaks:**"
      ]
    }
  ]
}